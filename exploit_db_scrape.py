import sys, os, pickle, re
from functools import reduce
from pathlib import Path

import requests_html
from bs4 import BeautifulSoup as bs
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

from cve_thread import _ThreadDistributedResponseRetriever, ChunkedRetriever
import cve_core
from cve_core import *

whitespace = re.compile('\W{2,}')
exploit_link_match = re.compile('(?<=/exploits/)\d+')
exploit_link_full_format = 'https://www.exploit-db.com/exploits/{}'

class ExploitDBRetriever(_ThreadDistributedResponseRetriever):
	def __init__(self,urls=None,**kw):
		if urls is None:
			max_exploit_number = get_last_exploit_number()
			urls = range(1,max_exploit_number+1)
		super().__init__(urls, **kw)
	
	def process_content(self,response):
		return get_exploit_details_from_response(response)
	
	def format_url(self,url):
		return exploit_link_full_format.format(url)

class ChunkedEDBRetriever(ChunkedRetriever, ExploitDBRetriever): pass
	def __init__(self,*a,**kw):
		super().__init__(*a, path='edb scrape', **kw)
# 	def bad_status_resolution(self):
# 		"""
# 		get the most recent bad-status page
# 		get the link to the next good page from there
# 		pop items off the queue until we reach the page before that one
# 		"""
# 		#index,url,response.status_code
# 		most_recent_bad_status = max(self.get_bad_status_responses(),key = lambda s: s[0])
# 		i,u,s = most_recent_bad_status
# 		while not self.success[i]:
# 			i-=1
# 		
# 		...
	
# 	def predict_bad_status(self,url,content):
# 		p = get_next_exploit_page_from_url_and_content(url,content)
# 		if int(p) - url > 1:
# 			return True
# 		return False
		
		
def get_next_exploit_page_from_url_and_content(url,content):
	s=bs(content)
	a,=list(filter(lambda t: 'title' in t.attrs and t['title']=='Next Exploit', s.find_all('a')))
	return a['href'].split('/')[-1]

def get_exploit_details_from_response(r):
	#print('get_exploit_details_from_response')
	s = bs(r.content.decode('utf-8'),features='lxml')
	
	cards = s.find_all('div',{'class':'card card-stats'})
	
	col_6_boxes = (card.find_all('div',{'class':'col-6 text-center'}) for card in cards)
	fields = list(map(get_h4_h6_fields,(stat for col_6_box in col_6_boxes for stat in col_6_box)))
	
	#CVE can have multiple listings, separated by whitespace (spaces and returns)
	fields[1][1] = whitespace.subn(', ',fields[1][1])[0]
	
	fields.extend(get_is_verified_and_has_app(cards))
	fields.append(('title',s.find('h1').text.strip().lower()))
	
	return fields

def get_exploit_details(exploit_number,session=None):
	if session is None:
		no_session=True
		session=requests_html.HTMLSession()
	
	else:
		no_session=False
	
	r=session.get(f'https://www.exploit-db.com/exploits/{exploit_number}')
	r.raise_for_status()
	#r.html.render()	#from testing, this seems unnecessary
	
	result = get_exploit_details_from_response(r)
	
	if no_session:
		session.close()
	
	return result

def get_h4_h6_fields(col_6_text_center_tab):
	h4, h6 = col_6_text_center_tab.findChildren()[:2]
	return [h4.text.strip()[:-1], h6.text.strip()]

def get_is_verified_and_has_app(cards):
	verified_tab = cards[0].find('div',{'class':'stats h5 text-center'})
	verified = get_h5_image(verified_tab)['class'][-1].endswith('check')
	app_tab = cards[2].find('div',{'class':'stats h5 text-center'})
	has_app = bool(get_h5_image(app_tab))
	return ('verified',verified),('has_app',has_app)

def get_h5_image(h5_tab):
	return h5_tab.findChild('i')

def get_last_exploit_number():
	session = requests_html.HTMLSession()
	try:
		r = session.get('https://www.exploit-db.com')
		r.raise_for_status()
		r.html.render()
		#print(r.html.links)
		links = filter(None, (exploit_link_match.search(l) for l in r.html.links))
		exploit = max(links, key=int)
	except Exception as e:
		print('error raised in `get_last_exploit_number`:',repr(e))
		exploit=48235
	else:
		return max(links, key = lambda s:int())
	finally:
		session.close()
	return exploit

def make_table(results):
	"""
	results is expected to be a list of len-2 ordered iterables in (key:value) format,
	the result of [get_exploit_details(id) for id in <id set>]
	"""
	data = np.array(list(filter(None,results)),dtype=object)
	#AXES: exploit id, attribute_type, qtype (field/value)
	fields = data[:,:,0]
	unique_fields = np.unique(fields.astype(str),axis=0).squeeze()
	assert np.array_equal(unique_fields,fields[0])
	values = data[:,:,1]
	assert values.shape == fields.shape
	fields = unique_fields
	df = pd.DataFrame(values,columns=fields)
	
	df['EDB-ID'] = df['EDB-ID'].astype(int)
	df.set_index('EDB-ID', inplace=True)
	
	for c in ('has_app','verified'):
		df[c] = df[c].astype(bool)
	df.loc[df['CVE'].apply(lambda v: not bool(re.search('\d+[-]\d+',str(v)))).values,'CVE'] = '--'
	
	df.to_csv(EDB_TABLE_PATH.format(low=df.index[0],high=df.index[-1]),sep='\t')
	
	return df

#not verified, no app:	48235	False, False
#verified, no app:		48224	True, False
#not verified, app:		48208	False, True
#verified, app:			48223	True, True

def get_valid_exploit_urls():
	try:
		with open(cve_core.EXPLOIT_DB_URLS_FILE) as f:
			data=eval(','.join(l[l.index(':')+1:-1] for l in f.readlines()))
		print(len(data),len(set(data)))
		return sorted(set(data))
	except FileNotFoundError:
		print('exploit db urls are not present; run exploit_db_url_retrieve.py')

def get_urls_not_downloaded(low=None, high=None):
	downloaded = np.array(get_merged_tables().index)
	if low is None and high is None: low,high = downloaded[0],downloaded[-1]
	downloaded = set(downloaded[(downloaded>=low) & (downloaded<=high)])
	
	urls = get_valid_exploit_urls()
	urls = set(urls[urls.index(low):urls.index(high)+1])
	
	in_range_but_not_downloaded = urls - downloaded
	
	return in_range_but_not_downloaded

def get_minimal_file_span():
	directory = Path(EDB_TABLE_PATH).parent
	files = list(filter(lambda f: f.endswith('.file'), os.listdir(directory)))
	"""lows, highs = zip(*(f[:-5].split('-') for f in files))
	lows = [int(l) for l in lows]
	highs = [int(h) for h in highs]
	
	flh = np.array(list(zip(files,lows,highs)),dtype=object)
	
	del lows,highs
	
	files = []
	while len(flh)>0:
		lowf = flh[flh[:,1]==np.min(flh[:,1])]
		highf = lowf[lowf[:,2]==np.max(lowf[:,2])].squeeze()
		files.append(highf[0])
		low = np.max(flh[flh[:,1]<=highf[-1],1])
		if low==highf[1]:
			try: low = np.min(flh[flh[:,1]>low,1])
			except ValueError: break
		flh = flh[flh[:,1] >= low]
	"""
	return directory, files
	

def get_merged_tables():
	table_dir, files = get_minimal_file_span()
	files = sorted(files, key=lambda f: np.diff(np.array(f[:-5].split('-'),dtype=int)))
	table = pd.read_csv(table_dir / files[0], delimiter='\t', index_col = 0)
	for f in files[1:]:
		t = pd.read_csv(table_dir / f, delimiter='\t', index_col = 0)
		#overlap = t.index <= table.index.max()
		#if overlap.any(): table = pd.concat((table,t[~overlap]))
		#else: table = pd.concat((table,t))
		remain = set(t.index) - set(table.index)
		if remain: table = pd.concat((table,t.loc[list(remain)]))
	table.sort_index(inplace=True)
	return table

def save_merged_table():
	df = get_merged_tables()
	df.to_csv(EDB_TABLE_PATH.format(low=df.index[0],high=df.index[-1]),sep='\t')

def check_results():
	#print(get_last_exploit_number())
	if True:
		urls = get_valid_exploit_urls()
		print(f'urls: <size={len(urls)}>')
		
		start = 0
		size = 200  #len(urls)-start
		urls = urls[start:start+size]
		
		print('exploit db scrape: start, size =',start,size)
	else:
		urls = get_urls_not_downloaded()
		size = len(urls)
	
	rtype = ExploitDBRetriever # ChunkedEDBRetriever
	r = rtype(
		urls, test_conn_err=False, threads=30, # 10
		#chunksize = 20
	)
	r.retrieve()
	
	print('* * TABLE * * ')
	print(make_table(r.get_processed_results(filt=True)))
	
	#print('response times:')
	#print(np.sort(r.response_times))
	threads = r.get_thread_count()
	plt.gca().axvline(threads,ls='--',c='k')
	plt.plot(range(size),r.response_times)
	plt.plot(
		range(threads,size),
		[np.average(r.response_times[i-threads:i])
		 for i in range(threads,size)]
	)
	plt.title('response time')
	plt.show()
	
	plt.hist(r.response_times)
	plt.title('response time')
	plt.show()

def compare_results():
	urls = get_valid_exploit_urls()
	
	start = 0
	size = 100  #len(urls)-start
	urls = urls[start:start+size]
	
	r2 = ChunkedEDBRetriever(
		urls, test_conn_err=False, threads=30, # 10
		chunksize = 10, count_time=False
	)
	r2.retrieve()
	
	r1 = ExploitDBRetriever(
		urls, test_conn_err=False, threads=30, # 10
		count_time=False
	)
	r1.retrieve()
	
	df1 = make_table(r1.get_processed_results(filt=True))
	df2 = make_table(r2.get_processed_results(filt=True))
	
	common_indices = set(df1.index) & set(df2.index)
	
	print('test 1:',df1.loc[common_indices].equals(df2.loc[common_indices]))
	print('test 2:',(df1.loc[common_indices] == df2.loc[common_indices]).values.all())
	print('test 3:',len(df1.index) == len(df1.index.unique()),len(df2.index) == len(df2.index.unique()))
	print('test 4:', len(df1.index) == len(df2.index))
	print('test 5:', len(df1.index) == size, len(df2.index) == size)

def check_chunks(_id = '1585608685.728714'):
	files = [f for f in os.listdir(f'chunks/{_id}') if f.endswith('.pickle')]
	
	for i,f in enumerate(files):
		with open(f'chunks/1585608685.728714/{f}','rb') as s: files[i]=pickle.load(s)
	
	joined = reduce(lambda a,b: list.__add__(a,b), files)
	
	t = make_table(joined)
	T = get_merged_tables()
	
	common_indices = set(t.index) & set(T.index)
	common_indices
	print('pickle test 1:',t.loc[common_indices].equals(T.loc[common_indices]))
	print('pickle test 2:',(t.loc[common_indices] == T.loc[common_indices]).values.all())
	return t

if __name__=='__main__':
	#check_results()
	#compare_results()
	check_chunks()